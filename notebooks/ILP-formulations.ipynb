{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow src folder to be imported from this notebook\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "module_path = str(Path.cwd().parents[0])\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ILP Formulation to cover an MSA with Blocks\n",
    "___\n",
    "\n",
    "- [X] Select minimum set of blocks covering the MSA\n",
    "- [X] Create a graph from the selected blocks\n",
    "- [X] Visualize the graph\n",
    "- [X] Output the graph in gfa format\n",
    "- [ ] join not covered positions by maximal blocks (no trivial blocks unless is needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "from src.blocks import Block\n",
    "from src.msa import AnalyzerMSA\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# pangenome graph\n",
    "from src.graph import (\n",
    "    nodes_edges_from_blocks, \n",
    "    PlotGraph\n",
    ")\n",
    "from src.graph.bandage_labels_from_gfa import bandage_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSA\n",
    "NAME_MSA = \"toyexample\"\n",
    "amsa = AnalyzerMSA()\n",
    "path_msa = f\"../msas/{NAME_MSA}.fa\"\n",
    "align, n_seqs, n_cols = amsa.load_msa(path_msa)\n",
    "n_seqs, n_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(align[0,1:2].seq)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Blocks for each position (r,c) in the MSA** \n",
    "\n",
    "$({r},c,c,MSA[r,c])$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks_one_char = []\n",
    "for col in range(n_cols):\n",
    "    for row in range(n_seqs):\n",
    "        blocks_one_char.append(\n",
    "            Block(K=(row,), i=col, j=col, label=align[row,col])\n",
    "        )\n",
    "\n",
    "blocks_one_char[:3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set of blocks from the  decomposition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load set of decomposed blocks\n",
    "path_blocks = f\"../experiment/block_decomposition/{NAME_MSA}.json\"\n",
    "\n",
    "with open(path_blocks) as fp:\n",
    "    decomposed_blocks = [Block(*block) for block in json.load(fp)] \n",
    "\n",
    "decomposed_blocks[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_coverage_panel(n_seqs, n_cols, blocks):\n",
    "    \"\"\"returns a matrix of size equal to msa (n_seq x n_cols) with \n",
    "    the number of blocks in the list_blocks that covers each position\"\"\"\n",
    "\n",
    "    # coverage_by_pos = defaultdict(int)\n",
    "    coverage_by_pos = np.zeros((n_seqs, n_cols))\n",
    "    for block in blocks:\n",
    "        for r in block.K:\n",
    "            for c in range(block.i,block.j+1):\n",
    "                coverage_by_pos[r,c] += 1\n",
    "    return coverage_by_pos\n",
    "    \n",
    "import numpy as np \n",
    "coverage_by_pos = check_coverage_panel(n_seqs, n_cols, decomposed_blocks)\n",
    "coverage_by_pos\n",
    "\n",
    "def get_missing_blocks(coverage_by_pos): \n",
    "    \"\"\"return the missing blocks to cover the MSA\n",
    "    all consecutives one character not covered positions are\n",
    "    clustered in one block\n",
    "    \"\"\"\n",
    "    rows, cols=np.where(coverage_by_pos == 0)\n",
    "    missing_blocks = [(r,c) for r,c in zip(rows,cols)]\n",
    "    # missing_blocks = get_missing_blocks(coverage_by_pos)\n",
    "    missing_blocks = sorted(missing_blocks, key= lambda d: (d[0],d[1]))\n",
    "    idx_missing_blocks_by_seq = defaultdict(list)\n",
    "    for pos in missing_blocks: \n",
    "        idx_missing_blocks_by_seq[pos[0]].append(pos[1])\n",
    "    # return get_missing_blocks\n",
    "    return idx_missing_blocks_by_seq\n",
    "\n",
    "\n",
    "def get_consecutive_pos(positions: list[int]):\n",
    "    \"given a list with positions, split it in sublists of consecutive numbers\"\n",
    "    rv = []\n",
    "\n",
    "    # Set up current list with first element of input\n",
    "    curr = [positions[0]]\n",
    "\n",
    "    # For each remaining element:\n",
    "    for x in positions[1:]:\n",
    "        # If the next element is not 1 greater than the last seen element\n",
    "        if x - 1 != curr[-1]:\n",
    "            # Append the list to the return variable and start a new list\n",
    "            rv.append(curr)\n",
    "            curr = [x]\n",
    "        # Otherwise, append the element to the current list.\n",
    "        else:\n",
    "            curr.append(x)\n",
    "    rv.append(curr)\n",
    "    return rv\n",
    "\n",
    "# now split each row into separate sublist of (row,col)\n",
    "missing_blocks=[]\n",
    "idx_missing_blocks_by_seq = get_missing_blocks(coverage_by_pos)\n",
    "for seq, cols in idx_missing_blocks_by_seq.items():\n",
    "    consecutive_pos = get_consecutive_pos(cols)\n",
    "    for pos in consecutive_pos: \n",
    "        label = str(align[int(seq)].seq[pos[0]:pos[-1]+1]) #if pos[0]!=pos[-1] else align[seq,pos[0]]\n",
    "        missing_blocks.append(\n",
    "        Block(K=(seq,), i=pos[0],j=pos[-1], label=label)\n",
    "        )\n",
    "\n",
    "missing_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set B: input blocks (maximal blocks, the decompositions under intersection by pairs and blocks of one position in the MSA)\n",
    "set_B = decomposed_blocks + blocks_one_char + [block for block in missing_blocks if block.j-block.i+1 > 1]\n",
    "\n",
    "# set_B = blocks_one_char\n",
    "# set_B.extend([Block((0,),0,0,\"A\"), Block((2,),4,4,\"A\")])\n",
    "\n",
    "# write idx for blocks \n",
    "blocks = [(\",\".join([str(r) for r in b.K]), b.i, b.j) for b in set_B] # (K,i,j)\n",
    "\n",
    "# write idx for MSA positions (row, col)\n",
    "msa_positions = [(r,c) for r in range(n_seqs) for c in range(n_cols)] \n",
    "\n",
    "# dictionary to store the string of each block indexed as Gurobipy uses it\n",
    "strings_ = { (\",\".join([str(_) for _ in b.K]), b.i, b.j): b.label for b in set_B}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_intersection(block1,block2):\n",
    "    \"intersection of blocks \"\n",
    "    block1_K = block1[0].split(\",\")\n",
    "    block2_K = block2[0].split(\",\")\n",
    "\n",
    "    # check for not empty intersection, otherwise skip to the next block1 in the list\n",
    "    common_rows = list(set(block1_K).intersection(set(block2_K))) # intersection set K\n",
    "    common_cols = list(set(range(block1[1],block1[2]+1)).intersection(set(range(block2[1],block2[2]+1)))) # intersection columns [i,j]\n",
    "\n",
    "    return True if common_rows and common_cols else False\n",
    "\n",
    "# example\n",
    "block1 = (\"1,2\",1,1)\n",
    "block2 = (\"3\",1,3)\n",
    "check_intersection(block1, block2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = gp.Model(\"pangeblocks\")\n",
    "\n",
    "# define variables\n",
    "C = model.addVars(blocks, vtype=GRB.BINARY, name=\"C\")\n",
    "U = model.addVars(msa_positions, vtype=GRB.BINARY, name=\"U\")\n",
    "\n",
    "# Constraints: \n",
    "for r,c in msa_positions:\n",
    "\n",
    "    # subset of blocks that covers the position [r,c]\n",
    "    subset_C = [ C[K,i,j] for K,i,j in blocks if str(r) in K.split(\",\") and i<=c<=j ]\n",
    "    if (r,c) in [(0,0),(2,4)]:\n",
    "        print(f\"{(r,c)}:len {len(subset_C)}\")\n",
    "    if len(subset_C)>0:\n",
    "        # print(f\"{len(subset_C)} blocks cover the position {(r,c)}\")\n",
    "        \n",
    "        ## 1. each position in the MSA is covered ONLY ONCE\n",
    "        model.addConstr( U[r,c] <= sum(subset_C), name=f\"constraint1({r},{c})\")\n",
    "        \n",
    "        ## 2. each position of the MSA is covered AT LEAST by one block\n",
    "        model.addConstr( U[r,c] >= 1, name=f\"constraint2({r},{c})\")\n",
    "\n",
    "\n",
    "## 3. overlapping blocks cannot be chosen\n",
    "# sort all blocks, \n",
    "blocks = sorted(blocks, key=lambda b: b[1]) # sort blocks by the starting position (K,start,end)\n",
    "\n",
    "# and analyze the intersections while update the constraints\n",
    "names_constraint3=[]\n",
    "for pos1,block1 in enumerate(blocks[:-1]):\n",
    "    # compare against the next blocks in the sorted list\n",
    "    for rel_pos, block2 in enumerate(blocks[pos1+1:]):\n",
    "        pos2 = rel_pos + pos1 + 1\n",
    "        block2 = blocks[pos2]\n",
    "        \n",
    "        # check for not empty intersection, otherwise, skip to the next block  \n",
    "        # note: set K is a string with the rows concatenated by a \",\" (due to Gurobi requirements to index the variables)\n",
    "        block1_K = block1[0].split(\",\")\n",
    "        block2_K = block2[0].split(\",\")\n",
    "\n",
    "        # check for not empty intersection, otherwise skip to the next block1 in the list\n",
    "        common_rows = list(set(block1_K).intersection(set(block2_K))) # intersection set K\n",
    "        common_cols = list(set(range(block1[1],block1[2]+1)).intersection(set(range(block2[1],block2[2]+1)))) # intersection columns [i,j]\n",
    "\n",
    "        if (common_rows and common_cols):\n",
    "            \n",
    "            # if the blocks intersect, then create the restriction \n",
    "            K1,i1,j1=block1\n",
    "            K2,i2,j2=block2\n",
    "            name_constraint=f\"constraint3({K1},{i1},{j1})-({K2},{i2},{j2})\"\n",
    "            model.addConstr(C[block1] + C[block2] <= 1 , name=name_constraint)\n",
    "            names_constraint3.append(name_constraint)\n",
    "\n",
    "# Objective function\n",
    "model.setObjective(C.sum('*','*','*'), GRB.MINIMIZE)\n",
    "\n",
    "model.optimize()\n",
    "\n",
    "Path(\"ilp-models\").mkdir(exist_ok=True)\n",
    "model.write(f\"ilp-models/{NAME_MSA}.lp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"constraint3(1,2,0,3)-(0,1,1,5)\" in names_constraint3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution_C = model.getAttr(\"X\", C)\n",
    "solution_U = model.getAttr(\"X\",U)\n",
    "len(solution_C)>0, len(solution_U)>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_blocks = []\n",
    "for k,v in solution_C.items(): \n",
    "    K,i,j=k\n",
    "    if v > 0:\n",
    "        used_blocks.append(\n",
    "            Block(eval(f\"({K},)\"),i,j, strings_[K,i,j])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_solution=sorted(used_blocks, key=lambda b: b.i)\n",
    "len(sorted_solution), sorted_solution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import astuple \n",
    "list_nodes = []\n",
    "list_edges = []\n",
    "\n",
    "for pos1, block1 in enumerate(sorted_solution[:-1]):\n",
    "    for rel_pos, block2 in enumerate(sorted_solution[pos1+1:]):\n",
    "        pos2 = rel_pos + pos1 + 1\n",
    "\n",
    "        nodes, edges = nodes_edges_from_blocks(block1, block2)\n",
    "        list_nodes.extend(nodes)\n",
    "        list_edges.extend(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k,i,j,b=list_edges[0].node1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_list_nodes = set([(node.K,node.i,node.j,node.label) for node in list_nodes])\n",
    "_list_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_list_edges = []\n",
    "for edge in list_edges:\n",
    "    node1=edge.node1\n",
    "    node2=edge.node2\n",
    "\n",
    "    _list_edges.append(\n",
    "        ((node1.K,node1.i,node1.j,node1.label),(node2.K,node2.i,node2.j,node2.label), tuple(edge.seqs))\n",
    "    )\n",
    "\n",
    "_list_edges = list(set(_list_edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_list_edges[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.flow.plot_graph import PlotGraph\n",
    "dg = PlotGraph()\n",
    "dg(nodes=list_nodes, edges=list_edges, path_save=f\"plots/{NAME_MSA}.dot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg.digraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Block((2,),4,5,\"GA\") in set_B"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# graph in GAF format\n",
    "lines_gfa = []\n",
    "\n",
    "HEADER = f\"H\\t{NAME_MSA}\"\n",
    "\n",
    "# segments (nodes)\n",
    "lines_segments = [] \n",
    "node2id={} # will be useful to add the lines/edges\n",
    "for id_node, node in enumerate(_list_nodes):\n",
    "    lines_segments.append(\n",
    "        f\"S\\t{id_node}\\t{node[-1]}\"\n",
    "    )\n",
    "\n",
    "    node2id[node]=id_node\n",
    "\n",
    "# links (edges)\n",
    "lines_links=[]\n",
    "data_paths = defaultdict(list)\n",
    "for edge in _list_edges:\n",
    "    node1, node2, seqs = edge[0], edge[1], edge[2]\n",
    "    lines_links.append(\n",
    "        f\"L\\t{node2id[node1]}\\t+\\t{node2id[node2]}\\t+\\t0M\"\n",
    "    )\n",
    "\n",
    "    # paths\n",
    "    for seq in seqs:\n",
    "        data_paths[seq].extend([node1,node2])\n",
    "\n",
    "lines_paths=[]\n",
    "paths = dict()\n",
    "for seq, nodes_seq in data_paths.items():\n",
    "    nodes_seq = list(set(nodes_seq))\n",
    "    id_nodes = [node2id[node] for node in sorted(nodes_seq, key=lambda node: node[1])]\n",
    "    paths[seq] = \",\".join([str(id_node)+\"+\" for id_node in id_nodes])\n",
    "    lines_paths.append(\n",
    "        f\"P\\tseq{seq}\\t\\t{paths[seq]}\"\n",
    "    )\n",
    "lines_gfa.append(HEADER)\n",
    "lines_gfa.extend(list(set(lines_segments)))\n",
    "lines_gfa.extend(list(set(lines_links)))\n",
    "lines_gfa.extend(lines_paths)\n",
    "\n",
    "path_gfa = Path(f\"../experiment/gfa/{NAME_MSA}.gfa\")\n",
    "path_gfa.parent.mkdir(exist_ok=True, parents=True)\n",
    "with open(path_gfa,\"w\") as fp:\n",
    "    for line in lines_gfa:\n",
    "        fp.write(line + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bandage labels\n",
    "bandage_labels(path_gfa, path_save_labels=f\"../experiment/gfa/{NAME_MSA}_bandage_labels.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_list_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_list_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow src folder to be imported from this notebook\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "module_path = str(Path.cwd().parents[0])\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Block(K=(0,), i=0, j=0, label='A'), Block(K=(2,), i=4, j=4, label='G')]\n",
      "Gurobi Optimizer version 9.5.2 build v9.5.2rc0 (linux64)\n",
      "Thread count: 32 physical cores, 64 logical processors, using up to 32 threads\n",
      "Optimize a model with 1026 rows, 43 columns and 2078 nonzeros\n",
      "Model fingerprint: 0x56c8bd8f\n",
      "Variable types: 0 continuous, 43 integer (43 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 1e+00]\n",
      "Found heuristic solution: objective 16.0000000\n",
      "Presolve removed 1026 rows and 43 columns\n",
      "Presolve time: 0.00s\n",
      "Presolve: All rows and columns removed\n",
      "\n",
      "Explored 0 nodes (0 simplex iterations) in 0.01 seconds (0.00 work units)\n",
      "Thread count was 1 (of 64 available processors)\n",
      "\n",
      "Solution count 2: 6 16 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 6.000000000000e+00, best bound 6.000000000000e+00, gap 0.0000%\n",
      "Not consecutive blocks\n",
      "Condicion- consecutive blocks\n",
      "Not consecutive blocks\n",
      "Not consecutive blocks\n",
      "Not consecutive blocks\n",
      "Condicion- consecutive blocks\n",
      "Not consecutive blocks\n",
      "Not consecutive blocks\n",
      "Not consecutive blocks\n",
      "Condicion- consecutive blocks\n",
      "Condicion- consecutive blocks\n",
      "Not consecutive blocks\n",
      "Not consecutive blocks\n",
      "Not consecutive blocks\n",
      "Condicion- consecutive blocks\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from src.blocks import Block\n",
    "from src.ilp.input import InputBlockSet\n",
    "from src.ilp.optimization import Optimization\n",
    "from src.ilp.variaton_graph_parser import asGFA\n",
    "NAME_MSA = \"toyexample\"\n",
    "# Load set of decomposed blocks\n",
    "path_blocks = f\"../experiment/block_decomposition/{NAME_MSA}.json\"\n",
    "path_msa=f\"../msas/{NAME_MSA}.fa\"\n",
    "\n",
    "with open(path_blocks) as fp:\n",
    "    blocks = [Block(*block) for block in json.load(fp)] \n",
    "\n",
    "inputset_gen = InputBlockSet()\n",
    "inputset = inputset_gen(path_msa, blocks)\n",
    "\n",
    "opt = Optimization(blocks=inputset, path_msa=path_msa)\n",
    "opt_coverage = opt()\n",
    "\n",
    "parser=asGFA()\n",
    "parser(opt_coverage,path_gfa=f\"../experiment/gfa/{NAME_MSA}.gfa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constraint = model.getConstrByName(\"constraint2(1,28)\")\n",
    "# print(f\"{model.getRow(constraint)} {constraint.Sense} {constraint.RHS}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pbenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7a3f744a40f2cc2bb6ed47f09e67b858fce5d2f0f12c5fcd35c0d04a62dbf281"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
